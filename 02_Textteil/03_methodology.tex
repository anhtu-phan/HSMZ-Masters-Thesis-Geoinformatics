\chapter{Methodology}
\label{ch:methodology}

This feasibility study is based on two essential parts.
The first is a reference implementation, providing insights on the work necessary to arrive at the base functionality.
Based on the implementation, an analysis of the application's functionality is made, an overview of the time spent on development and there is a qualitative review of the resulting codebase and a reflection on the work process.


\section{Reference implementation}
\label{sec:reference-implementation}

To produce a valid test subject for the proposal, the reference implementation is created according to a prior selection of tools and methods deemed appropriate for the task at hand.
The choice is made from the concepts and tools presented in \autoref{ch:conceptualfoundations}.

First, possible candidates are identified through internet search and then at least three candidates are selected using the number of \textquote{Stars} received on GitHub as an indicator of popularity.
In some cases, another metric has to be used where the technology itself predates GitHub (e.g.\ databases or programming languages) and its popularity should be judged by other means.
In this case, there is a yearly developer survey being conducted by the popular technology forum Stack Overflow with over 90.000 participants for 2023~\parencite{stackOverflowPoll} and the \textquote{State Of JS} survey with over 20.000 participants that is more focused on web development~\parencite{stateOfJSSurvey}.
Additionally, GitHub is publishing a yearly statistic on its public repositories, which is helpful for identifying technological trends and popularity among millions of open-source repositories~\parencite{stateOfTheOctoverse23}.
The selection is further narrowed by focusing on specific requirements that the study posits towards its supporting technology.

The decision on choosing a specific candidate is then made not by popularity alone, but with a stronger weight on a good fit to the project's requirements and needs.
If a less popular framework fits the specific style of development, it is preferred to the status quo.
Another case might be a more recent project that hasn't collected as high of a rating on GitHub, but presents a promising new paradigm or feature set.

The application development works from the most basic boilerplate code towards finding the appropriate structure for the specific use-case.
Well-known and easily defined components are built first and the special functionality is then built on top in constant cycles of adding functionality, reviewing the codebase and refactoring towards abstraction and separation of basics from specifics.
As there is only a rough architectural model for the project defined beforehand, tests and documentation is written later in the process, as the parts stabilise on their own and in their relationship among each other.
This method does not strictly adhere to common development procedures, but borrows loosely from agile development (sprints, review, reorientation) and simple forms of the ideas put forward in the book \textquote{Pattern-Oriented Software Architecture}, such as layering, separation, and standardised messaging~\parencite{patternOrientedSoftwareArchitecture}.
A more rigid structure for the development process might be desirable for teams, but the various and disparate \textquote{moving parts} in conjunction with heavy reliance on browser-only \ac{API}s complicate the creation of a well-simulated testing environment using either real or mock-data.

The application is implemented in its entirety, documented and packaged.
Appropriate test coverage is provided for the core functionality in the API and the messaging components, and the overall time spent is logged in timesheets and categorised by the general work areas.
The application's server components are deployed early on to university hardware and made available over the internet.
The client application is then run and tested on various consumer computer systems and networking setups.


\section{Analysis and evaluation}
\label{sec:analysis-evaluation}

A statistical analysis of the timesheets provides insight into the time spent on various aspects of the software.
It should differentiate between basic boilerplate code that can be reused and custom code used for the actual use case, in order to provide insights both into the feasibility of setting up such a system from scratch with only a single developer, as well as the potential cost of just reworking the parts deemed transient and related to the specific use-case.

The application's performance is tested regarding the load put on the \ac{CPU} (server and client) as well as the network throughput and latency.
It is verified that all message processing works as expected through unit testing and simple testing tasks performed on the application.
A practical test that analyses the actual user experience and using performers and real dance interaction is beyond the scope of this study.

The \emph{code quality} is assessed based on volume (lines of code without comments) and cyclomatic complexity, which counts the number of linearly independent paths throughout a piece of code \parencite[see][]{mcCabeComplexity}.
Both metrics should be kept low, as code with a high number lines makes it harder to read and a high complexity is harder to follow and understand.
It is recommended to limit the cyclomatic complexity to a value of 10, with special justified exceptions going up to 15 as a maximum \parencite[15]{testingCyclomaticComplexity}, in order to facilitate a development style that aims to reduce the danger of potential errors caused by excessive complexity.
The code volume is difficult to reduce to a static maximum, but \textquote{the [\ac{SATC}] has found the most effective evaluation is a combination of size and complexity} \parencite[6]{softwareMetricsReliability}, so a self-imposed threshold should be set, even if exceptions are made later on.
For \ac{OOP}, it is often advised to limit a file to a single class, but depending on the language used, the file can also just contain multiple functions that are better grouped together instead of scattered across multiple files.
A general rule of thumb that has proven to be a good measurement from past experience is that a single file should not exceed 160-200 lines of code (without comments), in order to be able to quickly skim it and get a general overview.
There are exceptions to these rules where a specific function or class exceeds this threshold, but breaking it up would not make it easier to understand. In those cases, there can either be a discussion about how to change the general application design to partition the functionality differently, or it can be decided to keep this code as is and improve documentation.
Reducing volume and complexity is especially important if the code should be passed on to other people that want to maintain and modify it for further use. Here, the general idea is that the transient (hackable) parts (e.g. \ac{UI}) should be as simple as possible, avoiding unnecessary complexity, and the more static and stable core parts can be the location where the more complex parts is moved to (e.g. the core \ac{SDK}).

Additionally, a \emph{critical reflection} and analysis of the development process should weigh the expectations against the experiences made during the implementation of the decisions made in planning the application.
It should evaluate the notion of general reproducibility, feasibility and discuss the benefits and drawbacks of establishing a task-specific application from scratch.
