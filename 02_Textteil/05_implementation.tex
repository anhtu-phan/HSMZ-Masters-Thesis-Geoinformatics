\chapter{Implementation}

\section{Project Setup}

The supporting \emph{server infrastructure} was deployed first to allow development on a working remote WebRTC infrastructure. The basis was a clean, freshly bootstrapped Kubernetes installation running on a single server with 16 CPU-cores (with multithreading), 64GB RAM and a 512GB SSD drive, located at Mainz University and connected to the internet via a one gigabit network connection.
LiveKit and its Redis database were installed via the application deployment manager Helm, using an official installation from its maintainers.
To simplify the deployment, LiveKit was placed behind a reverse proxy to manage SSL termination via the LetsEncrypt service, as well as routing to the actual service running inside the cluster.
However, this simplified setup required LiveKit to be configured to listen on a single TCP port instead of a range of UDP ports, as it would usually be deployed.
The potential downside of this deployment configuration was deemed insignificant, since the service does not have to scale to more than a handful of users.
The detailed Kubernetes setup instructions are documented in the according folder in the Git repository \footnote{Kubernetes setup instructions: \href{https://github.com/dasantonym/sensorama/tree/master/kubernetes}{https://github.com/dasantonym/sensorama/tree/master/kubernetes}}.

The \emph{LiveKit} installation was deployed with only slight deviation from the default configuration.
It was set up to use TCP as a transport protocol to allow easier integration with SSL termination using the reverse-proxy.
Otherwise, the configuration defined the endpoints for sending webhook requests and custom credentials for making requests to it via the server-side \ac{SDK} and generating valid access tokens for users to connect to rooms.
The Helm chart used for the installation set up the system along its Redis database installation in a single command and the server was immediately ready for connections.

The development process was conducted in a desktop environment, using the suite of tools developed by JetBrains (WebStorm, PyCharm and CLion), as these are free for educational use and provide a complete out-of-the-box-environment for development including debugging, smart code-completion, versioning, containerisation and deployment.
A local Docker Desktop installation provided the possibility to run services and databases locally to support development before publishing to the production environment.
Versioning was done via Git on the GitLab platform provided by Mainz University for the Rhineland-Palatinate \footnote{\url{https://gitlab.rlp.net}}.

\section{API server}

The first implementation, the \ac{API} server, was generated using the Feathers \ac{CLI} tool with standard username and password authentication and WebSockets as well as HTTP transports enabled.
It implements the core services for Users, Spaces, Tokens and LiveKit events.
These services were also autogenerated using the Feathers \ac{CLI} utility and were used largely unmodified, except for adding the properties on the models for Users and Spaces as defined in \autoref{sec:datamodeling}.
A custom service class was added for the Tokens, as these do not persist in the database, but are generated on the fly by the LiveKit server \ac{SDK}.
All other boilerplate code for the \ac{API}, including the MongoDB integration, the authentication mechanism, as well as the REST and WebSockets transport integrations were also autogenerated using the \ac{CLI}.
An additional custom LivekitEvent service was added to the API and allowed it to receive webhook requests via HTTP from the LiveKit server containing updates on connecting and disconnecting users.
These events were then not persisted, but relayed to the real-time channels that the users are connected to.
The channels feature provided by Feathers was used to automatically subscribe connecting users to updates on the services for Spaces and Livekit events.

\section{Core SDK}

To make the base code independent of the use-case, the basic functionality was bundled in an \ac{NPM} module, which can be used in other projects.
This module contains the abstract classes \textquote{DataProducer} and \textquote{HeadTracker} alongside the message specification for the various types of transmitted data.
It was written to propagate updates via events instead of the reactive patterns used in Vue so that it can also be used independently from the framework used for the study.
Unit tests were added to the module to support a stable implementation.

\section{User interface}

The Quasar framework provides a \ac{CLI} to generate new projects, which allowed to select basic implementation details (e.g. language, state management) and then produced a complete and working empty Vue project with sample components that served as the starting point for the \ac{UI} implementation.
The first thing added to the project was the library \textquote{feathers-pinia}\footnote{\url{https://feathers-pinia.pages.dev}}, which is provided by the Feathers developer community and promises easy integration of an existing Feathers \ac{API} with any Vue project using the Pinia state management system.
The extension was integrated by linking the client library that the Feathers server project automatically generates and then configuring basic authentication settings.
The routing configuration and page-components were set up according to the sitemap.

The overview page for the spaces is a list of the names alongside the currently connected users and buttons to either join as a participant or to passively view it as a spectator.
On joining a space, the user first needs to activate their microphone to allow the audio context to be activated.
The user is then presented with three basic control panels.
The \textquote{Data producer} panel allows setting a URL of a local WebSockets server published by a data producer, setting the message type received from it and enabling a tracing function to log the packet transmission statistics.
Once connected, the panel switches to showing a preview of the incoming points data, transmission statistics and a button to disconnect.
Internally, the panel creates a reactive data store that instantiates the \textquote{DataProducer} class from the core \ac{SDK}, watches incoming message events and populates the received data as reactive properties to be used across the \ac{UI} in various other components without the need of instantiating more class instances.
The second panel, \textquote{Head tracker} works in the sae way but uses the Web Bluetooth \ac{API} to select a nearby device and connect to it to receive its data messages.
The third component is used to configure the sonification by setting thresholds on incoming movement quality values.
Once a threshold is crossed, a sound event is triggered.

When using the page to view a space, there is only a single component, the \textquote{SpaceViewer} that shows a \ac{3D} room in which all incoming points are rendered as small spheres, giving the impression of a human figure and for each participant there is a differently coloured light source that follows the centre of mass of the points associated with the participant.
The viewer also renders all sonifications and audio streams at their respective spatial positions.
This allows the user to view the \ac{3D} scene on screen while listening to binaural audio or to use the built-in \ac{VR} functionality to experience the scene in an immersive way.

\section{Data producers}

The general \emph{data producer} was written in Python and provides multiple data sources: an interface to a Blazepose implementation on the Oak-D \ac{3D} camera, as well as reading depth images as point clouds from the camera and an interface to load and playback motion capture data in the \ac{BVH} file format.
All three data sources were implemented as separate Python classes, because the classes related to the Oak-D camera were built by modifying existing code for pose recognition \parencite{githubDepthAiBlazePose} and pointcloud processing \parencite{githubDepthAiPointcloud}.
The \ac{BVH} data producer however, was created from scratch.
The separate producer classes were set up to all support calling a function to return current point data as a multidimensional array in a loop that is quantised to a fixed framerate of 25 frames.
The returned data is then packed as a byte sequence and sent over websockets according to the appropriate format (see \autoref{sec:datamodeling}).
This way, the disparate sources could all be imported in a single main file that uses a combined set of utility functions for running a WebSockets server and for packing data.

Due to the lack of a Python-based client for the Captury Live system, the Captury producer had to be implemented separately as a C++ project using CMake as a build system and based on the \textquote{RemoteCaptury} client library \parencite{githubRemoteCaptury}, as well as an example project for a WebSockets server implementation in C++ \parencite{githubCppWebSocketsDemo}.

The custom built head tracking device was implemented as an Arduino project and as such, was first realised as a hardware setup and then outfitted with a custom built firmware written in the Arduino-specifc flavour of C/C++.
The hardware implementation was created using the \textquote{Arduino Connect RP2040}\footnote{\url{https://docs.arduino.cc/hardware/nano-rp2040-connect/}}, which is based on the Raspberry PI 2040 microcontroller and has an onboard BluetoohLE module.
The IMU module used was the \textquote{9-DOF Absolute Orientation IMU Fusion Breakout}\footnote{\url{https://www.adafruit.com/product/4646}} which uses the BNO055 chip produced by Bosch\footnote{\url{https://www.bosch-sensortec.com/products/smart-sensor-systems/bno055/}}.
This chip has the benefit of already pre-processing the data coming from the gyroscope, accelerometer and magnetometer into an absolute world position which can be directly read out from the breakout board via the \ac{I2C} bus.
As a third component, a small 3.7V lithium battery was added alongside a charging module\footnote{\url{https://www.adafruit.com/product/1905}}.
Only six connections needed to be soldered between the three modules (2x charger and 4x \ac{IMU}) and the resulting circuit was ready to function as a custom head tracker.
For the software implementation, the basic example code for the Adafruit module was used to set up a continuous polling of the positioning module, reading the values for position and device calibration status and sending them as byte sequences at a fixed rate of ~25fps over BluetoothLE.
