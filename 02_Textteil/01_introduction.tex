\chapter{Introduction}
\label{chapter:introduction}

\section{Background}

In recent years, remote collaboration has become increasingly important due to the rise of broader digitalisation strategies, especially since the pandemic. As a result, teleconferencing and telepresence platforms have become more pervasive in many work environments. These platforms allow people to work together remotely in real-time, usually focusing on streaming video and audio, document sharing or collaborative whiteboarding. While this is fine for most use cases and desktop-based workplaces, it lacks certain immersive qualities required for practices such as contemporary dance, where people relate to a shared space and physical presence. This became apparent in March 2020, when dancers could no longer rehearse and work together due to the lockdown. Despite this, there were attempts at using Zoom to stream and record collaborative rehearsals or dance classes. Still, these were confined to a screen-centric interface and limited to audio and video.

While commercial conferencing tools such as Zoom, Google Meet, and Microsoft Teams dominate in popularity among conferencing applications \parencite{mostPopularConferencingPlatforms}, there are both free and open-source variants such as JitsiMeet and BigBlueButton. However, these all focus on the most basic form of screen-based conferencing mentioned above. While there are various domain-specific solutions for specialised applications, mainly in telemedicine (as well as general industry and the military), that support more immersive and task-specific remote collaboration, these are either unaffordable or unavailable to the general public.

As support for web standards is driven by key industry players \parencite{pushingInteroperabilityForward}, and with it the availability of a wide range of basic functionality, as well as access to display and sensor technology for deploying applications on desktop and mobile devices, there is an increased potential for smaller and more specific applications to be built and deployed with relative ease. This opens up new possibilities for niche cases of remote collaboration, such as dance, where collaborative functionality needs to be extended from the traditional paradigm of remotely viewing video streams to the creation of shared virtual environments that facilitate a sense of \textquote{being there} together, if only at an abstract level \parencite{surveyOfPresence}.

The standard for \ac{RTC} in Browsers or \ac{WebRTC} \parencite{webRtcSpec} was first proposed by Google in 2011 and became an official \ac{W3C} standard in 2021 \parencite{webRtcOfficialWebStandard}. It is already being used in a wide range of applications, such as some of the conferencing tools mentioned above, media streaming servers such as Wowza or Ant, or real-time frameworks and servers such as Mediasoup, Janus or LiveKit. In its most basic form, \ac{WebRTC} establishes peer-to-peer connections between different devices, allowing low-latency exchange of media streams and arbitrary messages on so-called data channels.

\section{Proposal}

I propose a feasibility study for a reference implementation of a domain-specific telepresence platform based entirely on web standards and open-source components. The platform allows streaming of sensor data in addition to audio and video. This means that remote collaborators can share a wide range of data from multiple sources, such as motion capture, wearables and more. As the data flows through the WebRTC data channels alongside the usual video and audio streams, it can be analysed and rendered as required for a specific task on the client device.

A reference implementation will demonstrate the platform's capabilities to simulate essential aspects of presence in a shared virtual environment \parencite{surveyOfPresence}. The software implementation, written entirely in \ac{JS}, should rely on existing open-source libraries and frameworks as much as possible and add as little custom code as necessary. Only the minimal viable product will be built to observe its basic functionality, leaving issues of robustness, interface design, security and scalability outside the scope of this study.

For dancers to communicate while moving in remote rehearsal spaces, a video feed is less critical as they need to be free from looking at a screen or typing on a keyboard. A virtual or augmented reality headset is also ineffective because it hinders vision and movement in the physical space. Here, spatial sonification seems the optimal alternative to creating a virtual environment without interfering with the dancers' ability to move freely. The sound generation code should be kept as minimal as possible.

For practical implementation testing, two remote rehearsal spaces of similar size will need to be set up with a motion capture system (TDB) that generates spatial data from the dancers' movements. Both rooms will be within the university network to provide a known and controllable quality of service. The two dancers will use the system in each location. The interaction will be via wireless in-ear sports headsets, commonly used for exercise and providing basic verbal communication via \ac{WebRTC}'s audio channels.

To interact with the spatial dimension of a shared virtual environment consisting of both rehearsal rooms, sonification is used as a means of orientation in the virtual space. The Web Audio \ac{API} provides functionalities for generating, mixing and positioning sound sources in spatial audio \parencite{webAudioSpec}, which will be used to render each participant's microphone sound alongside their distinct signature sound (one percussive and the other sustained) at their tracked position in space, modulated by two basic movement qualities, the contraction index and the quantity of movement, based on an existing proposal for qualitative movement analysis \parencite{movementQualities} and derived from the motion capture data. Each participant can only hear the other to avoid confusion. The spatial audio representation should provide a basic sense of position, while the sound characteristic should indicate the type of movement being performed. In addition, verbal cues can be exchanged.

A separate client implementation uses the same data to visualise the dancers' motion capture data as basic \ac{3D} images. It adds the combined sonification, which can then be viewed by a third party using the \ac{XR} Device \ac{API} for the web \parencite{webXrSpec}, allowing viewers to evaluate aesthetic aspects of the combined interaction result. In addition, a recording of the session's streams can later be replayed and reviewed by the dancers using this client implementation.
